# -*- coding: utf-8 -*-
"""A5_DPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1SHkK0eQGpsIFtMNtaL8pMCxc0971Iy
"""

pip install datasets transformers trl huggingface_hub

import torch
import os
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from huggingface_hub import login, whoami

# Get Hugging Face token from environment
hf_token = "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"  # Replace with your actual token
if not hf_token.startswith("hf_"):
    raise ValueError("Invalid Hugging Face token. Please check and set a valid token.")

print("✅ Using HF Token:", hf_token[:10] + "..." + hf_token[-5:])  # Print part of the token for verification

# Verify token before logging in
try:
    print("✅ Hugging Face User Info:", whoami(hf_token))
except Exception as e:
    raise ValueError("❌ Invalid Hugging Face token. Please check and set a valid token.") from e

# Login to Hugging Face
login(token=hf_token)

# Load dataset
try:
    dataset = load_dataset("Anthropic/hh-rlhf", split="train")
    print("✅ Dataset loaded successfully!")
except Exception as e:
    raise ValueError("❌ Error loading dataset. Check dataset name and internet connection.") from e

# Load model and tokenizer
model_name = "facebook/opt-1.3b"  # Use a smaller model if needed (e.g., "gpt2")
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f"✅ Tokenizer for {model_name} loaded successfully!")
except Exception as e:
    raise ValueError(f"❌ Error loading tokenizer for {model_name}.") from e

# Define model initialization function
def model_init():
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name)
        print(f"✅ Model {model_name} initialized successfully!")
        return model
    except Exception as e:
        raise ValueError(f"❌ Error initializing model {model_name}.") from e

# Instantiate the model explicitly
model = model_init()

# Define DPOConfig (training arguments)
dpo_config = DPOConfig(
    beta=0.1,  # Regularization term
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_steps=100,
    save_steps=1000,
    save_total_limit=2,
    output_dir="./results",
    eval_strategy="steps",
    logging_dir="./logs",
)

# Train with DPO
try:
    trainer = DPOTrainer(
        model=model,  # Pass initialized model directly
        ref_model=None,  # If you don't have a separate reference model
        args=dpo_config,
        train_dataset=dataset,
        eval_dataset=dataset,
        tokenizer=tokenizer,  # Ensure tokenizer is passed
    )
    print("✅ Training started...")
    trainer.train()
    print("✅ Training completed successfully!")
except Exception as e:
    raise ValueError("❌ Error during training. Check model parameters and dataset.") from e

# Save and upload model
try:
    trainer.model.push_to_hub("your_huggingface_token")  # Replace with your actual Hugging Face repo
    print("✅ Model uploaded successfully!")
except Exception as e:
    raise ValueError("❌ Error uploading model. Ensure you have the correct permissions.") from e